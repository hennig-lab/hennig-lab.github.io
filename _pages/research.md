---
layout: page
title: Research Topics
permalink: /research/
---

Our lab's goal is to understand how animals and people can use the same set of neurons to learn and perform so many different behaviors. To do this, we take three general approaches:

- __theory__ (e.g., normative models: "How _should_ someone perform this task?")
- __machine learning__ (e.g., artificial agents trained with reinforcement learning)
- __neural data analysis__ (e.g., statistical models of high-dimensional neural activity)

## How do neural populations support multiple behaviors?

We have human patients perform a variety of different tasks, spanning multiple cognitive, sensory, and motor domains. Tasks include video games like Pacman as well as classic psychophysics tasks.

Alongside this, we conduct __24/7 recordings of spiking activity in human hippocampus and cortex across multiple days__.

<img src="{{ site.baseurl }}/assets/images/papers/subspace.png" width="500px">

Some questions we're interested in:
- Do correlations between neurons remain stable even across unrelated tasks?
- Can we predict a neuron's activity during a task before the patient has even started the task?

Our lab is in close collaboration with the labs of Sameer Sheth, Ben Hayden, Nicole Provenza, Eleonora Bartoli, and Sarah Heilbronner at Baylor College of Medicine.

## How do animals learn from reward?

We are interested in understanding reinforcement learning in the dopamine system, using recurrent neural network (RNN) models, and recordings from neural populations in the prefrontal cortex, amygdala, and basal ganglia.

Some questions we're interested in:
- What is the neural basis of __meta-learning__, or "learning to learn"?
- How do neurons develop representations of __uncertainty__?
- How do animals rapidly learn new reward associations?

You can browse our [publications](/publications) for summaries of our work in these areas.

<!-- Learning can be defined simply as systematic improvements in performance at a given task. As for how changes in the brain actually give rise to learning, there are many key questions. What objective is the brain optimizing, and how is this selected? What are the learning rules? What are the constraints? -->

<!-- ## Meta-learning -->

<!-- ## Emergence of probabilistic representations -->
